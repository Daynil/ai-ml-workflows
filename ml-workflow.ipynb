{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning Workflow\n",
    "\n",
    "A very high level outline of a typical machine learning workflow.\n",
    "\n",
    "The goal is to keep this *very* concise and only include detail where it makes sense. I want to be able to use this as my standard reference for when I'm working on a new ML project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "pd.options.display.max_columns = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 sample datasets will give us a broad overview of the two main subtypes of classic ML problems\n",
    "* `heart_disease` is a classification dataset (predicting whether someone has heart disease or not).\n",
    "* `boston_df` is a regression dataset (predicting the median house prices of cities in Boston)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_disease = pd.read_csv('./data/heart-disease.csv')\n",
    "boston_df = pd.read_csv('./data/boston.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = features, y = target (industry standard variables)\n",
    "X = heart_disease.drop('target', axis=1)\n",
    "y = heart_disease['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Consider the limitations of random splitting for many datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick a suitable model and train\n",
    "Pick a model to use. [sklearn ML cheat sheet](https://scikit-learn.org/stable/tutorial/machine_learning_map/index.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "\n",
    "clf_model = RandomForestClassifier()\n",
    "regr_model = RandomForestRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0,\n",
       "        0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 1,\n",
       "        1, 0, 0, 0, 0, 0, 0, 1, 1, 1], dtype=int64),\n",
       " array([[0.61, 0.39],\n",
       "        [0.71, 0.29],\n",
       "        [0.09, 0.91],\n",
       "        [0.82, 0.18],\n",
       "        [0.97, 0.03],\n",
       "        [0.52, 0.48],\n",
       "        [0.59, 0.41],\n",
       "        [0.24, 0.76],\n",
       "        [0.23, 0.77],\n",
       "        [0.99, 0.01],\n",
       "        [0.42, 0.58],\n",
       "        [0.92, 0.08],\n",
       "        [0.2 , 0.8 ],\n",
       "        [0.14, 0.86],\n",
       "        [0.71, 0.29],\n",
       "        [1.  , 0.  ],\n",
       "        [0.61, 0.39],\n",
       "        [0.17, 0.83],\n",
       "        [0.82, 0.18],\n",
       "        [0.3 , 0.7 ],\n",
       "        [0.3 , 0.7 ],\n",
       "        [0.89, 0.11],\n",
       "        [0.97, 0.03],\n",
       "        [0.1 , 0.9 ],\n",
       "        [0.01, 0.99],\n",
       "        [0.07, 0.93],\n",
       "        [0.14, 0.86],\n",
       "        [0.82, 0.18],\n",
       "        [0.94, 0.06],\n",
       "        [0.24, 0.76],\n",
       "        [0.83, 0.17],\n",
       "        [0.89, 0.11],\n",
       "        [0.06, 0.94],\n",
       "        [0.01, 0.99],\n",
       "        [0.88, 0.12],\n",
       "        [0.38, 0.62],\n",
       "        [0.41, 0.59],\n",
       "        [0.37, 0.63],\n",
       "        [0.01, 0.99],\n",
       "        [0.1 , 0.9 ],\n",
       "        [0.43, 0.57],\n",
       "        [0.  , 1.  ],\n",
       "        [0.94, 0.06],\n",
       "        [0.54, 0.46],\n",
       "        [0.83, 0.17],\n",
       "        [0.08, 0.92],\n",
       "        [0.18, 0.82],\n",
       "        [0.22, 0.78],\n",
       "        [0.65, 0.35],\n",
       "        [0.19, 0.81],\n",
       "        [0.87, 0.13],\n",
       "        [0.79, 0.21],\n",
       "        [0.07, 0.93],\n",
       "        [0.2 , 0.8 ],\n",
       "        [0.94, 0.06],\n",
       "        [0.17, 0.83],\n",
       "        [0.  , 1.  ],\n",
       "        [0.02, 0.98],\n",
       "        [0.63, 0.37],\n",
       "        [0.87, 0.13],\n",
       "        [0.36, 0.64],\n",
       "        [0.95, 0.05],\n",
       "        [0.22, 0.78],\n",
       "        [0.94, 0.06],\n",
       "        [0.93, 0.07],\n",
       "        [0.33, 0.67],\n",
       "        [0.49, 0.51],\n",
       "        [0.97, 0.03],\n",
       "        [0.83, 0.17],\n",
       "        [0.98, 0.02],\n",
       "        [0.76, 0.24],\n",
       "        [0.76, 0.24],\n",
       "        [0.59, 0.41],\n",
       "        [0.  , 1.  ],\n",
       "        [0.37, 0.63],\n",
       "        [0.16, 0.84]]))"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.fit(X_train, y_train)\n",
    "\n",
    "y_preds = clf_model.predict(X_test)\n",
    "y_probs = clf_model.predict_proba(X_test)\n",
    "\n",
    "y_preds, y_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate the model\n",
    "\n",
    "Every scikit-learn model has a default metric accessible through the `score()` function. However, there area a range of different metrics you can use depending on the model you're using.\n",
    "\n",
    "See https://scikit-learn.org/stable/modules/model_evaluation.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8289473684210527"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf_model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.85245902 0.8852459  0.80327869 0.81666667 0.76666667]\n",
      "[0.78947368 0.90625    0.82758621 0.81818182 0.76315789]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# scoring=None uses default score() metric\n",
    "print(cross_val_score(\n",
    "    estimator=clf_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring=None\n",
    "))\n",
    "print(cross_val_score(\n",
    "    estimator=clf_model,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    cv=5, # 5-fold cross-validation\n",
    "    scoring='precision'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy\n",
      "0.8289473684210527\n",
      "roc auc\n",
      "0.8291666666666666\n",
      "confusion matrix\n",
      "[[30  6]\n",
      " [ 7 33]]\n",
      "classification report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.83      0.82        36\n",
      "           1       0.85      0.82      0.84        40\n",
      "\n",
      "    accuracy                           0.83        76\n",
      "   macro avg       0.83      0.83      0.83        76\n",
      "weighted avg       0.83      0.83      0.83        76\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Other classification metrics\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print('accuracy')\n",
    "print(accuracy_score(y_test, y_preds))\n",
    "\n",
    "# Receiver Operating Characteristic (ROC curve)/Area under curve (AUC)\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "false_positive_rate, true_positive_rate, thresholds = roc_curve(y_test, y_probs[:, 1])\n",
    "print('roc auc')\n",
    "print(roc_auc_score(y_test, y_preds))\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('confusion matrix')\n",
    "print(confusion_matrix(y_test, y_preds))\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "print('classification report')\n",
    "print(classification_report(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r squared score\n",
      "0.9217375486209248\n",
      "MAE\n",
      "1.694323529411767\n",
      "MSE\n",
      "5.857417696078444\n"
     ]
    }
   ],
   "source": [
    "# Other regression metrics\n",
    "\n",
    "X = boston_df.drop('target', axis=1)\n",
    "y = boston_df['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "regr_model.fit(X_train, y_train)\n",
    "y_preds = regr_model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "print('r squared score')\n",
    "print(r2_score(y_test, y_preds))\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "print('MAE')\n",
    "print(mean_absolute_error(y_test, y_preds))\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print('MSE')\n",
    "print(mean_squared_error(y_test, y_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Improve through experimentation\n",
    "\n",
    "Two of the main methods to improve a model's baseline metrics (the first evaluation metrics you get) include improving the data and improving the model.\n",
    "\n",
    "### Improving the data\n",
    "* Can we collect more data? In ML, more data is generally better as it gives the model more opportunities to learn patterns.\n",
    "* Can we improve our data? This could be filling in missing values, finding a better encoding (turning things into numbers), feature engineering, etc.\n",
    "\n",
    "### Improving the model\n",
    "* Is there a better model we could use?\n",
    "* Could we improve the current model with **hyperparameter tuning**?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap': True,\n",
       " 'ccp_alpha': 0.0,\n",
       " 'class_weight': None,\n",
       " 'criterion': 'gini',\n",
       " 'max_depth': None,\n",
       " 'max_features': 'sqrt',\n",
       " 'max_leaf_nodes': None,\n",
       " 'max_samples': None,\n",
       " 'min_impurity_decrease': 0.0,\n",
       " 'min_samples_leaf': 1,\n",
       " 'min_samples_split': 2,\n",
       " 'min_weight_fraction_leaf': 0.0,\n",
       " 'n_estimators': 100,\n",
       " 'n_jobs': None,\n",
       " 'oob_score': False,\n",
       " 'random_state': None,\n",
       " 'verbose': 0,\n",
       " 'warm_start': False}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# You can get a list of adjustable hyperparameters\n",
    "clf_model.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8157894736842105\n",
      "0.8289473684210527\n"
     ]
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters by hand\n",
    "\n",
    "# Split data into X & y\n",
    "X = heart_disease.drop(\"target\", axis=1) # use all columns except target\n",
    "y = heart_disease[\"target\"] # we want to predict y using X\n",
    "\n",
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Instantiate two models with different settings\n",
    "clf_1 = RandomForestClassifier(n_estimators=100)\n",
    "clf_2 = RandomForestClassifier(n_estimators=200)\n",
    "\n",
    "# Fit both models on training data\n",
    "clf_1.fit(X_train, y_train)\n",
    "clf_2.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate both models on test data and see which is best\n",
    "print(clf_1.score(X_test, y_test))\n",
    "print(clf_2.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=4, min_samples_split=4, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   4.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=6, n_estimators=1200; total time=   4.1s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.3s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=5, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=1, min_samples_split=4, n_estimators=200; total time=   0.7s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   4.3s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   4.2s\n",
      "[CV] END max_depth=None, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=1200; total time=   4.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.4s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=4, min_samples_split=2, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=4, min_samples_split=6, n_estimators=200; total time=   0.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   1.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=4, min_samples_split=6, n_estimators=500; total time=   1.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=1000; total time=   3.1s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=1000; total time=   3.8s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=1000; total time=   3.6s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=1000; total time=   3.5s\n",
      "[CV] END max_depth=30, max_features=log2, min_samples_leaf=2, min_samples_split=6, n_estimators=1000; total time=   3.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   0.2s\n",
      "{'n_estimators': 1000, 'min_samples_split': 2, 'min_samples_leaf': 4, 'max_features': 'sqrt', 'max_depth': 10}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7540983606557377"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of adjusting hyperparameters computationally (recommended)\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Define a grid of hyperparameters\n",
    "grid = {\"n_estimators\": [10, 100, 200, 500, 1000, 1200],\n",
    "        \"max_depth\": [None, 5, 10, 20, 30],\n",
    "        \"max_features\": [\"log2\", \"sqrt\"],\n",
    "        \"min_samples_split\": [2, 4, 6],\n",
    "        \"min_samples_leaf\": [1, 2, 4]}\n",
    "\n",
    "# Split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Set n_jobs to -1 to use all cores (NOTE: n_jobs=-1 is broken as of 8 Dec 2019, using n_jobs=1 works)\n",
    "clf = RandomForestClassifier(n_jobs=1)\n",
    "\n",
    "# Setup RandomizedSearchCV\n",
    "rs_clf = RandomizedSearchCV(estimator=clf,\n",
    "                            param_distributions=grid,\n",
    "                            n_iter=10, # try 10 models total\n",
    "                            cv=5, # 5-fold cross-validation\n",
    "                            verbose=2) # print out results\n",
    "\n",
    "# Fit the RandomizedSearchCV version of clf\n",
    "rs_clf.fit(X_train, y_train);\n",
    "\n",
    "# Find the best hyperparameters\n",
    "print(rs_clf.best_params_)\n",
    "\n",
    "# Scoring automatically uses the best hyperparameters\n",
    "rs_clf.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and reload a trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7540983606557377\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "file_name = 'rs_random_forest_model_1.pkl'\n",
    "pickle.dump(rs_clf, open(file_name, 'wb'))\n",
    "loaded_pickle_model = pickle.load(open(file_name, 'rb'))\n",
    "print(loaded_pickle_model.score(X_test, y_test))\n",
    "Path(file_name).unlink()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All together with simple model\n",
    "\n",
    "We can put a number of different sklearn functions together with `Pipeline`.\n",
    "\n",
    "For a machine learning model to work, you need to have no missing data and no non-numeric values. We can handle these in various ways, such as dropping, filling, encoding, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22188417408787875"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "\n",
    "# Import data and drop rows with missing labels\n",
    "data = pd.read_csv('./data/car-sales-extended-missing-data.csv')\n",
    "data.dropna(subset=['Price'], inplace=True)\n",
    "\n",
    "# Define different features and column transformer pipelines\n",
    "# NOTE: Make sure to process numerical categorical columns separately\n",
    "# E.g. if we had imputed 'Doors' here, we would fill it with 'missing'\n",
    "# Which is not only meaningless but would cause an error with the encoder\n",
    "# It also does not need to be encoded since it is already a number\n",
    "categorical_features = ['Make', 'Colour']\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "door_feature = ['Doors']\n",
    "door_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value=4))\n",
    "])\n",
    "\n",
    "numerical_features = ['Odometer (KM)']\n",
    "numerical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean'))\n",
    "])\n",
    "\n",
    "# Setup preprocessing steps (fill missing values, then convert to numbers)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('door', door_transformer, door_feature),\n",
    "        ('num', numerical_transformer, numerical_features)\n",
    "    ],\n",
    "    # NOTE: This allows you to know which col transform failed\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', RandomForestRegressor(\n",
    "        # Setting verbose here allows you to estimate time remaining\n",
    "        # Random forest can take a while for large datasets\n",
    "        verbose=3,\n",
    "        # For large datasets, use all processors (-1)\n",
    "        # This bogs down comp significantly (leave 1 processor if working?)\n",
    "        n_jobs=-1\n",
    "    ))\n",
    "])\n",
    "\n",
    "\n",
    "X = data.drop('Price', axis=1)\n",
    "y = data['Price']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
